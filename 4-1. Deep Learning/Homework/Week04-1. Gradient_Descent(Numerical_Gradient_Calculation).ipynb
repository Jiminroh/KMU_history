{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddedbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885f94a",
   "metadata": {},
   "source": [
    "# Loss function을 정의해 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b59fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y_pred, y_real):\n",
    "    result = np.sum((y_real-y_pred)**2)\n",
    "    return result\n",
    "\n",
    "def cross_entropy_error(y_pred, y_real, is_onehot = True):\n",
    "    if y_real.ndim==1:\n",
    "        y_real = y_real.reshape(1,y_real.size)\n",
    "        y_pred = y_pred.reshape(1,y_pred.size)\n",
    "\n",
    "    batch_size = y_pred.shape[0]\n",
    "    if is_onehot:\n",
    "        result = -np.sum(y_real * np.log(y_pred+1e-7)) / batch_size # 입실론 트릭\n",
    "    else:\n",
    "        result = -np.sum(np.log(y_pred[np.arange(batch_size), y_real] + 1e-7)) / batch_size\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe19b45-841a-4321-a2e9-684ac0b57d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(pred, target):\n",
    "    return np.sum((target-pred)**2)\n",
    "\n",
    "def cee(pred, target):\n",
    "    softmax_value = softmax(pred)\n",
    "    return -np.sum(target*np.log(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cae7f",
   "metadata": {},
   "source": [
    "# 수치 미분 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c2650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad(f, x):\n",
    "    h = 1e-4\n",
    "    result = (f(x+h) - f(x-h))/(2*h)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38a9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편미분 \n",
    "def numerical_grad_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        x_i = x[idx]\n",
    "        \n",
    "        x[idx] = x_i + h\n",
    "        f_x_plus_h = f(x)\n",
    "        \n",
    "        x[idx] = x_i-h\n",
    "        f_x_minus_h = f(x)\n",
    "        \n",
    "        x[idx] = x_i\n",
    "        grad[idx] = (f_x_plus_h - f_x_minus_h)/(2*h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def numerical_grad_2d(f,x): \n",
    "    if x.ndim == 1:\n",
    "        grad = numerical_grad_1d(f,x)\n",
    "    else:\n",
    "        grad = np.zeros_like(x)\n",
    "        for idx in range(x.shape[0]):\n",
    "            grad[idx] = numerical_grad_1d(f,x[idx])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c1d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20480714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_grad_1d(my_func, np.array([0.0,2.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb82e1",
   "metadata": {},
   "source": [
    "# Gradient Descent 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9c127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_weight, lr=0.01, step_num=100):\n",
    "    weight = init_weight\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_grad2d(f,weight)\n",
    "        weight = weight - lr*grad\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70bb44",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 다음과 같은 과정을 수행해보자.\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>1. 간단한 neural network class작성하기</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>2. 이 neural network에서 numerical gradient calculation으로 gradient descent하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07bc0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from act_fn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c1935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 간단한 neural network 작성하기\n",
    "\n",
    "class SimpleNet:\n",
    "    # 이 SimpleNet은 입력층의 뉴런이 2개, 출력층 뉴런이 3개인 NN\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "        \n",
    "    # 2차원 데이터 x가 들어오면 3차원 데이터인 result 반환\n",
    "    def predict(self, x):\n",
    "        result = np.dot(x, self.W)\n",
    "        result = softmax_prev(result)\n",
    "        print(result.shape)\n",
    "        return result\n",
    "    \n",
    "    def loss(self, x, y_real, is_onehot=True):\n",
    "        y_pred = self.predict(x)\n",
    "        loss = cross_entropy_error(y_pred, y_real)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6f26b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight (W)\n",
      "[[-0.06366021 -0.30344345  0.410768  ]\n",
      " [ 0.50913737  0.27970113  0.28726009]]\n",
      "\n",
      "(3,)\n",
      "NN result with softmax activation\n",
      "[0.35802331 0.25220331 0.38977338]\n",
      "\n",
      "Cross Entropy Loss\n",
      "(3,)\n",
      "0.9421895387204898\n"
     ]
    }
   ],
   "source": [
    "net=SimpleNet()\n",
    "print('Initial weight (W)')\n",
    "print(net.W)\n",
    "print()\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "y = np.array([0,0,1])\n",
    "\n",
    "p = net.predict(x)\n",
    "print('NN result with softmax activation')\n",
    "print(p)\n",
    "print()\n",
    "\n",
    "print('Cross Entropy Loss')\n",
    "print(net.loss(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e35ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# 2. Gradient Descent 수행하기\n",
    "f = lambda w: net.loss(x, y)\n",
    "grad = numerical_grad_2d(f, net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44791279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.W.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788ced66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21481393,  0.15132195, -0.36613588],\n",
       "       [ 0.3222209 ,  0.22698292, -0.54920382]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875f714",
   "metadata": {},
   "source": [
    "# 이제 MNIST를 분류하는 FFNN을 직접 학습해봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0ac1a",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.2em;line-height:1.5em'>다음과 같은 과정을 수행해야 합니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>1. neural network class 정의하기: 여기선, Two-layer FFNN을 정의하겠습니다.이 클래스에는 다음과 같은 속성과 메소드가 필요합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>(1)params 속성: </b>Network의 weight들이 들어있는 dictionary.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>(2)predict() 메소드: </b>Forward Propagation을 진행하는 메소드</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>(3)get_loss() 메소드: </b>predict()의 결과와 실제 레이블을 비교하여 cross entropy loss를 계산하는 메소드</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>(4)get_grad() 메소드: </b>현재 W상태에서 gradient를 산출하는 메소드</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>(5)get_accuracy() 메소드: </b>(Optional) 현재 상태에서 네트워크가 inference하는 결과의 정확도를 산출하는 메소드</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>2. Mini-batch 데이터를 데이터에 입력</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>3. 현재 W상태에서 gradient를 계산</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>4. gradient descent로 learning_rate만큼 이동</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a8b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from act_fn import *\n",
    "from mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d8f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = dict()\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        result = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        result = sigmoid_prev(result)\n",
    "        result = np.dot(result, self.params['W2']) + self.params['b2']\n",
    "        result = softmax_prev(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_loss(self, x, y_real, is_onehot):\n",
    "        y_pred = self.predict(x)\n",
    "        result = cross_entropy_error(y_pred, y_real, is_onehot=is_onehot)\n",
    "        return result\n",
    "    \n",
    "    def get_grad(self, x, y_real, is_onehot=True):\n",
    "        loss = lambda W: self.get_loss(x, y_real, is_onehot=is_onehot)\n",
    "        grads = dict()\n",
    "        grads['W1'] = numerical_grad_2d(loss, self.params['W1'])\n",
    "        grads['b1'] = numerical_grad_2d(loss, self.params['b1'])\n",
    "        grads['W2'] = numerical_grad_2d(loss, self.params['W2'])\n",
    "        grads['b2'] = numerical_grad_2d(loss, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7bd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_onehot = True\n",
    "(x_trn, y_trn), (x_tst, y_tst) = load_mnist(normalize=True, \n",
    "                                            flatten=True, \n",
    "                                            one_hot_label=is_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3fe073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> GEt Gradient\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_loss() missing 1 required positional argument: 'is_onehot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grad[key]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 학습 경과 기록\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m train_loss_list\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 1에폭 당 정확도 계산\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_loss() missing 1 required positional argument: 'is_onehot'"
     ]
    }
   ],
   "source": [
    "network = MyNet(784, 100, 10)\n",
    "\n",
    "iters_num = 10000  # 반복횟수\n",
    "train_size = x_trn.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # print(i)\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_trn[batch_mask]\n",
    "    t_batch = y_trn[batch_mask]\n",
    "\n",
    "    # 오차역전파법으로 기울기 계산\n",
    "    print('>> GEt Gradient')\n",
    "    grad = network.get_grad(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.get_loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1에폭 당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.get_accuracy(x_trn, y_trn)\n",
    "        test_acc = network.get_accuracy(x_tst, y_tst)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
