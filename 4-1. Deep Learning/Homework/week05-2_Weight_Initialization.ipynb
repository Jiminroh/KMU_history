{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2a6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95f803",
   "metadata": {},
   "source": [
    "## Initialization을 사용하는 방법: torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0c7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = nn.Linear(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a253ee8",
   "metadata": {},
   "source": [
    "### 각 모듈별로 weight나 bias객체 속성에 실제값이 들어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c802a2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3664, -0.2565, -0.5446],\n",
       "         [-0.3872, -0.3495, -0.3003]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3566, -0.5169], requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in fc1.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc73e4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3664, -0.2565, -0.5446],\n",
       "        [-0.3872, -0.3495, -0.3003]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112f177e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3566, -0.5169], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc756db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.init.constant_(tensor: torch.Tensor, val: float) -> torch.Tensor>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.constant_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648cfa3",
   "metadata": {},
   "source": [
    "### nn.init에는 여러가지 종류가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "761d0dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(fc1.weight, mean=0.0, std=1.0)\n",
    "nn.init.zeros_(fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3831cc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4745, -1.0326, -0.2501],\n",
       "        [ 0.4300, -0.9808, -1.2280]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0db2567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a5cce",
   "metadata": {},
   "source": [
    "### 직접 값을 지정하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff7fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_tensor = torch.tensor([[1.,2.,3.],[4.,5.,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4379e7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1.weight.data = tmp_tensor\n",
    "fc1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e1a14",
   "metadata": {},
   "source": [
    "## Xavier Initialization & He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238811bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.8362, -0.4360, -0.7508],\n",
       "        [ 0.0969, -0.6915,  0.6707]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_normal_(fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8dce0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6311,  0.3224, -0.5526],\n",
       "        [-2.4464,  0.1049, -0.8517]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.kaiming_normal_(fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360c6ae",
   "metadata": {},
   "source": [
    "# 실제 모델에 적용해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec94b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f0012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "transform = transforms.Compose([transforms.ToTensor(), # 이미지를 텐서로 변경하고\n",
    "                                transforms.Normalize((0.1307,), # 이미지를 0.1307, 0.3081값으로 normalize\n",
    "                                                     (0.3081,))\n",
    "                               ])\n",
    "\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d89bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**8\n",
    "trn_loader = DataLoader(trn_dset, batch_size = batch_size, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d527384",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = MyNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a91ee8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e26edf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.473391\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.421461\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.396257\n",
      "\n",
      "Test set: Average loss: 0.2863, Accuracy: 9159/10000 (92%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.294717\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.288874\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.225600\n",
      "\n",
      "Test set: Average loss: 0.2120, Accuracy: 9369/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.115410\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.124022\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.105217\n",
      "\n",
      "Test set: Average loss: 0.1824, Accuracy: 9449/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.121445\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.185661\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.202870\n",
      "\n",
      "Test set: Average loss: 0.1621, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.124781\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.127398\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.082861\n",
      "\n",
      "Test set: Average loss: 0.1426, Accuracy: 9567/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.191960\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.150181\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.178115\n",
      "\n",
      "Test set: Average loss: 0.1329, Accuracy: 9597/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.108400\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.057941\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.155267\n",
      "\n",
      "Test set: Average loss: 0.1247, Accuracy: 9621/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.056355\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.130912\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.112298\n",
      "\n",
      "Test set: Average loss: 0.1170, Accuracy: 9649/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.092926\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.099834\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.116557\n",
      "\n",
      "Test set: Average loss: 0.1115, Accuracy: 9667/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.070113\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.054550\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.159089\n",
      "\n",
      "Test set: Average loss: 0.1089, Accuracy: 9677/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        my_opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        my_opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cd43f",
   "metadata": {},
   "source": [
    "# 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b998a",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.2em;line-height:1.5em'>1. Weight와 bias를 전부 0으로 채워서 initialize할 때, 학습이 어떻게 진행되는지 살펴봅시다. 마찬가지로, 1로 채웠을때 어떻게 학습이 되는지 살펴봅시다.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714505d-a59b-4db8-94d2-5ef628f80936",
   "metadata": {},
   "source": [
    "1. Weight, bias 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02920ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.apply(self._init_weights) \n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): \n",
    "            nn.init.zeros_(submodule.weight) \n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.0) \n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e485ec48-f178-4557-b76d-d27b048b7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "690fe225-7b4f-42e3-8535-5e7dceadea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc061c9d-72b9-49eb-be8a-a9501060df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 2.302531\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 2.301696\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 2.302309\n",
      "\n",
      "Test set: Average loss: 2.3020, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 2.301799\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 2.301018\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 2.303010\n",
      "\n",
      "Test set: Average loss: 2.3017, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 2.299384\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 2.303166\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 2.299219\n",
      "\n",
      "Test set: Average loss: 2.3015, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 2.301749\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 2.301078\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 2.299443\n",
      "\n",
      "Test set: Average loss: 2.3013, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 2.299851\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 2.301536\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 2.303342\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 2.303274\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 2.301511\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 2.306088\n",
      "\n",
      "Test set: Average loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 2.301913\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 2.300883\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 2.300696\n",
      "\n",
      "Test set: Average loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 2.300740\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 2.301245\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 2.300046\n",
      "\n",
      "Test set: Average loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 2.302150\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 2.301171\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 2.305264\n",
      "\n",
      "Test set: Average loss: 2.3011, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 2.304300\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 2.299911\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 2.302827\n",
      "\n",
      "Test set: Average loss: 2.3010, Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        my_opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        my_opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90ce6d-14e9-4829-b38b-2c8484f0af0e",
   "metadata": {},
   "source": [
    "2. Weight, bias 1로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a41a0b4c-303c-46ef-acee-22c8cdd739de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.apply(self._init_weights) \n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): \n",
    "            nn.init.ones_(submodule.weight) \n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(1.0) \n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "893bda9e-98d3-4d21-b659-b9b46e386cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ba0d0e0-0166-4601-92a2-0bf97cbfd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e37f0c04-c907-4a61-8420-309be9af1acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 24.092194\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 15.945511\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 63.403255\n",
      "\n",
      "Test set: Average loss: 54.1661, Accuracy: 1203/10000 (12%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 35.120193\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 17.716738\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 31.196281\n",
      "\n",
      "Test set: Average loss: 26.5224, Accuracy: 1859/10000 (19%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 24.418856\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 27.992228\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 18.391039\n",
      "\n",
      "Test set: Average loss: 18.7788, Accuracy: 1560/10000 (16%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 21.226013\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 17.176117\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 15.184653\n",
      "\n",
      "Test set: Average loss: 13.0406, Accuracy: 1190/10000 (12%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 10.224091\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 10.563285\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 8.192807\n",
      "\n",
      "Test set: Average loss: 11.9156, Accuracy: 1278/10000 (13%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 10.143862\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 8.025168\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 4.840822\n",
      "\n",
      "Test set: Average loss: 9.5895, Accuracy: 1265/10000 (13%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 7.904822\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 4.772343\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 5.011455\n",
      "\n",
      "Test set: Average loss: 6.3427, Accuracy: 1668/10000 (17%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 13.295431\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 11.267438\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 9.398660\n",
      "\n",
      "Test set: Average loss: 7.6751, Accuracy: 1633/10000 (16%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 6.396872\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 5.909946\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 5.292999\n",
      "\n",
      "Test set: Average loss: 3.4912, Accuracy: 1467/10000 (15%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 4.000496\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 3.776993\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 4.170922\n",
      "\n",
      "Test set: Average loss: 4.3004, Accuracy: 1346/10000 (13%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        my_opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        my_opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76386ad-641f-4fda-8136-272cec5d8417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
