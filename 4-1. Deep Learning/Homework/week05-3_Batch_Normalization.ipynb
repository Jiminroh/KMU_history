{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a24bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0f417",
   "metadata": {},
   "source": [
    "## 실험을 CPU에서? GPU에서?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4001f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e108e",
   "metadata": {},
   "source": [
    "## 비교대상: BN없는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efb24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e1b1d",
   "metadata": {},
   "source": [
    "## BN이 적용된 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4262bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet_BN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.bn1 = nn.BatchNorm1d(100) # input -> hidden1로 가는 과정에서 필요한 batchnorm layer\n",
    "        self.bn2 = nn.BatchNorm1d(100) # hidden1 -> hidden2로 가는 과정에서 필요한 batchnorm layer\n",
    "#         self.bn3 = nn.BatchNorm1d(10) # output layer에서는 batchnorm이 통상적으로 잘 사용되지 않는 것 같습니다.\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x) # batchnorm은 affine연산(matrix multiplication)이후 사용.(activation전에!)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x) # batchnorm은 affine연산(matrix multiplication)이후 사용.(activation전에!)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216de1d6",
   "metadata": {},
   "source": [
    "## model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ae6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet().to(device)\n",
    "model_bn = MyNet_BN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00cb728-74f5-4c64-83ef-ce2047d17e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.0134, 0.9590, 1.0369, 1.0102, 1.0180, 1.0287, 1.0283, 0.9790, 0.9533,\n",
       "        0.9470, 1.0323, 1.0334, 0.9725, 1.0355, 1.0009, 0.9471, 1.0617, 0.9284,\n",
       "        0.9864, 1.0751, 0.9663, 1.0070, 1.0738, 0.9379, 1.0808, 0.9841, 0.9952,\n",
       "        0.9498, 1.0096, 0.9415, 1.0185, 1.0375, 0.9310, 0.9785, 1.0189, 1.0098,\n",
       "        0.9683, 0.9963, 0.9988, 0.9895, 0.9608, 1.0258, 1.0632, 0.9668, 1.0284,\n",
       "        1.0814, 0.9274, 1.0098, 0.9484, 0.9672, 1.0444, 0.9983, 0.9513, 0.9975,\n",
       "        1.0273, 0.9476, 0.9817, 0.9676, 0.9429, 1.0633, 0.9372, 0.9775, 0.9243,\n",
       "        0.9977, 1.0380, 1.0500, 1.0208, 1.0039, 0.9781, 0.9932, 1.0152, 1.0636,\n",
       "        0.9608, 0.9651, 0.9950, 1.0031, 1.0339, 1.0499, 0.9567, 0.9936, 1.0434,\n",
       "        1.0182, 0.9872, 1.0502, 0.9399, 0.9832, 1.0120, 1.0093, 1.0263, 1.0493,\n",
       "        1.0531, 0.9509, 0.9541, 1.0821, 0.9933, 0.9631, 0.9903, 1.0476, 0.9959,\n",
       "        0.9461], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bn.bn1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854fa49",
   "metadata": {},
   "source": [
    "## optimizer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3835e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(params = model.parameters(), lr = 2e-4)\n",
    "opt_bn = optim.Adam(params = model_bn.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d449d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "transform = transforms.Compose([transforms.ToTensor(), # 이미지를 텐서로 변경하고\n",
    "                                transforms.Normalize((0.1307,), # 이미지를 0.1307, 0.3081값으로 normalize\n",
    "                                                     (0.3081,))\n",
    "                               ])\n",
    "\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4a5963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**8\n",
    "trn_loader = DataLoader(trn_dset, batch_size = batch_size, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34548b76",
   "metadata": {},
   "source": [
    "# BN이 없는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298f85c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.554761\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.319640\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.395627\n",
      "\n",
      "Test set: Average loss: 0.3011, Accuracy: 9160/10000 (92%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.351479\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.206745\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.186450\n",
      "\n",
      "Test set: Average loss: 0.2154, Accuracy: 9369/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.224601\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.231334\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.240454\n",
      "\n",
      "Test set: Average loss: 0.1789, Accuracy: 9484/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.173836\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.169836\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.200259\n",
      "\n",
      "Test set: Average loss: 0.1532, Accuracy: 9554/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.128940\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.106160\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.103774\n",
      "\n",
      "Test set: Average loss: 0.1428, Accuracy: 9570/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.119031\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.079329\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.134702\n",
      "\n",
      "Test set: Average loss: 0.1299, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.115245\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.109223\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.067749\n",
      "\n",
      "Test set: Average loss: 0.1222, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.107699\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.156743\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.079272\n",
      "\n",
      "Test set: Average loss: 0.1132, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.098784\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.059705\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.119324\n",
      "\n",
      "Test set: Average loss: 0.1052, Accuracy: 9683/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.099960\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.077653\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.085293\n",
      "\n",
      "Test set: Average loss: 0.1010, Accuracy: 9687/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # batchnorm layer, dropout layer 할때 중요함 \n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch) \n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35104ff6",
   "metadata": {},
   "source": [
    "## BN이 있는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff83ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.767260\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.489565\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.363960\n",
      "\n",
      "Test set: Average loss: 0.3989, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.360913\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.317260\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.284415\n",
      "\n",
      "Test set: Average loss: 0.2535, Accuracy: 9346/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.205070\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.145415\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.208734\n",
      "\n",
      "Test set: Average loss: 0.1949, Accuracy: 9469/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.185326\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.153296\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.139370\n",
      "\n",
      "Test set: Average loss: 0.1631, Accuracy: 9547/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.168872\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.124869\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.103113\n",
      "\n",
      "Test set: Average loss: 0.1429, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.086627\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.125390\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.115491\n",
      "\n",
      "Test set: Average loss: 0.1282, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.098876\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.072585\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.158890\n",
      "\n",
      "Test set: Average loss: 0.1159, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.085827\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.092368\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.114789\n",
      "\n",
      "Test set: Average loss: 0.1088, Accuracy: 9686/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.117976\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.073165\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.104005\n",
      "\n",
      "Test set: Average loss: 0.1028, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.086991\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.087127\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.094376\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9712/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model_bn.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        opt_bn.zero_grad()\n",
    "        y_batch_prob = model_bn(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        opt_bn.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model_bn.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model_bn(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a7780",
   "metadata": {},
   "source": [
    "# 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a44a03",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.2em;line-height:1.5em'>1. BatchNorm을 하면 initialization에 크게 신경쓰지 않아도 되고, learning rate를 좀 크게 해도 관계 없다고 합니다. 실제로 그런지 확인해볼까요? Weight Initialization을 평균이 0, 표준편차가 0.2인 정규분포에서 random으로 추출하도록 하고, learning_rate를 0.01로 하고 실험을 해봅시다. BN을 했을때와 하지 않았을 때를 비교해보세요. Epoch에 따라 traning_error, test_error, test_accuracy를 모니터링한 결과를 알려주세요</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681798c-a427-4987-ac97-ad730932e9b9",
   "metadata": {},
   "source": [
    "### BN이 적용되지 않은 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd927b7-f995-4e5b-bc8b-38b01d71c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.apply(self._init_weights) \n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): \n",
    "            nn.init.normal_(submodule.weight, mean=0.0, std=0.2) \n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) \n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39601f30-d3d3-4ce8-823c-6c4a0a315be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BN이 적용된 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8363dd94-fbe5-4290-8d2d-cda2138c0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet_BN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "#         self.bn3 = nn.BatchNorm1d(10)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear):\n",
    "            nn.init.normal_(submodule.weight, mean=0.0, std=0.2)\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ddc3b-171c-4c64-ab42-1f765fe89f46",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06d9cf6e-1a9d-420f-ae8e-aaa5330a3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet().to(device)\n",
    "model_bn = MyNet_BN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95b609-0efc-4cea-b2e2-1ffd16aef4fb",
   "metadata": {},
   "source": [
    "### optimizer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22cb4f6c-a43d-4981-bb73-ef536a2829e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(params = model.parameters(), lr = 0.01)\n",
    "opt_bn = optim.Adam(params = model_bn.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec59d0-2641-4e8d-ba50-52b5e5c1fd80",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efb7f5df-3a02-41d6-89a8-6e52a6f736cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.264647\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.211953\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.317759\n",
      "\n",
      "Test set: Average loss: 0.1970, Accuracy: 9397/10000 (94%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.187084\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.172763\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.125752\n",
      "\n",
      "Test set: Average loss: 0.1805, Accuracy: 9453/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.189033\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.130665\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.162061\n",
      "\n",
      "Test set: Average loss: 0.1477, Accuracy: 9565/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.099336\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.138095\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.069682\n",
      "\n",
      "Test set: Average loss: 0.1491, Accuracy: 9598/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.084196\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.070680\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.128796\n",
      "\n",
      "Test set: Average loss: 0.1414, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.073626\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.055641\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.083763\n",
      "\n",
      "Test set: Average loss: 0.1514, Accuracy: 9607/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.015560\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.041685\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.032311\n",
      "\n",
      "Test set: Average loss: 0.1554, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.047333\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.140044\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.017121\n",
      "\n",
      "Test set: Average loss: 0.1396, Accuracy: 9627/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.061482\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.087674\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.011634\n",
      "\n",
      "Test set: Average loss: 0.1440, Accuracy: 9628/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.053203\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.063309\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.152131\n",
      "\n",
      "Test set: Average loss: 0.1233, Accuracy: 9691/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ebba20-617e-492a-a0c1-c96ba86add2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.179636\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.090706\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.053969\n",
      "\n",
      "Test set: Average loss: 0.1086, Accuracy: 9656/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.047467\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.113237\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.078898\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9679/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.036076\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.132586\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.108027\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9703/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.055547\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.051897\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.078810\n",
      "\n",
      "Test set: Average loss: 0.0882, Accuracy: 9748/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.029547\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.032664\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.073456\n",
      "\n",
      "Test set: Average loss: 0.0825, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.028769\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.075555\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.156979\n",
      "\n",
      "Test set: Average loss: 0.0906, Accuracy: 9762/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.023967\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.035813\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.073384\n",
      "\n",
      "Test set: Average loss: 0.0858, Accuracy: 9764/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.013468\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.029472\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.016297\n",
      "\n",
      "Test set: Average loss: 0.0799, Accuracy: 9788/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.020284\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.016127\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.143488\n",
      "\n",
      "Test set: Average loss: 0.1156, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.005490\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.012222\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.005064\n",
      "\n",
      "Test set: Average loss: 0.0993, Accuracy: 9744/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BN 적용 된 모델 \n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model_bn.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        opt_bn.zero_grad()\n",
    "        y_batch_prob = model_bn(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        opt_bn.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model_bn.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model_bn(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e7dbc-721c-4c06-bfef-96312f9f2010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
