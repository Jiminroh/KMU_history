{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e5806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728129b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08ee7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "transform = transforms.Compose([transforms.ToTensor(), # 이미지를 텐서로 변경하고\n",
    "                                transforms.Normalize((0.1307,), # 이미지를 0.1307, 0.3081값으로 normalize\n",
    "                                                     (0.3081,))\n",
    "                               ])\n",
    "\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a048d25",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c5bc8",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.4em;line-height:1.5em'>Dropout은 언제 써야될까요?</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>(1) Affine(Matrix Multiplication) - Activation - Dropout?</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>(2) Affine(Matrix Multiplication) - Dropout - Activation?</span>\n",
    "\n",
    "<span style = 'font-size:1.2em;line-height:1.5em'>확실하게 정해진 건 없습니다. 다만, 일반적으로 relu를 activation function으로 쓸때는 방법 (2)를, 나머지는 방법 (1)을 사용합니다.</span>\n",
    "\n",
    "참고: https://sebastianraschka.com/faq/docs/dropout-activation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8527075",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**8\n",
    "trn_loader = DataLoader(trn_dset, batch_size = batch_size, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ed974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.dropout = nn.Dropout(0.25) # Dropout layer 생성\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x) # ReLU함수를 쓸 때는 dropout을 activation전에 사용\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x) # ReLU함수를 쓸 때는 dropout을 activation전에 사용\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d656a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet()\n",
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92e1518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 1.025503\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.714304\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.716697\n",
      "\n",
      "Test set: Average loss: 0.3731, Accuracy: 8958/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.541710\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.338770\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.472699\n",
      "\n",
      "Test set: Average loss: 0.2670, Accuracy: 9231/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.331487\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.429821\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.251965\n",
      "\n",
      "Test set: Average loss: 0.2238, Accuracy: 9351/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.293357\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.275039\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.166580\n",
      "\n",
      "Test set: Average loss: 0.1941, Accuracy: 9418/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.281516\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.314426\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.263287\n",
      "\n",
      "Test set: Average loss: 0.1736, Accuracy: 9488/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.276675\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.251606\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.151089\n",
      "\n",
      "Test set: Average loss: 0.1578, Accuracy: 9523/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.267347\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.256897\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.206098\n",
      "\n",
      "Test set: Average loss: 0.1474, Accuracy: 9558/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.294348\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.236987\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.184612\n",
      "\n",
      "Test set: Average loss: 0.1380, Accuracy: 9580/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.271220\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.188452\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.216283\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.180530\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.139184\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.221770\n",
      "\n",
      "Test set: Average loss: 0.1226, Accuracy: 9607/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        my_opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        my_opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d876f",
   "metadata": {},
   "source": [
    "# Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c769b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dropout부분을 뺐습니다.\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.apply(self._init_weights) # 모델을 만들때, self._init_weights()를 호출하여 parameter 초기화\n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7e16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet().to(device)\n",
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-4, weight_decay=0.1) #L2 penalty에 들어가는 람다 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7f2716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.663717\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.366411\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.366536\n",
      "\n",
      "Test set: Average loss: 0.4202, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.429006\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.409038\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.486117\n",
      "\n",
      "Test set: Average loss: 0.3902, Accuracy: 9150/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.423686\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.416705\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.474556\n",
      "\n",
      "Test set: Average loss: 0.4091, Accuracy: 9150/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.399151\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.389580\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.547813\n",
      "\n",
      "Test set: Average loss: 0.4279, Accuracy: 9096/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.490711\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.465856\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.531822\n",
      "\n",
      "Test set: Average loss: 0.4499, Accuracy: 9071/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.402790\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.595161\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.473963\n",
      "\n",
      "Test set: Average loss: 0.4697, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.495887\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.484441\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.602786\n",
      "\n",
      "Test set: Average loss: 0.4782, Accuracy: 8994/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.470341\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.587902\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.447756\n",
      "\n",
      "Test set: Average loss: 0.4930, Accuracy: 8978/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.516838\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.458467\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.409820\n",
      "\n",
      "Test set: Average loss: 0.5003, Accuracy: 8939/10000 (89%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.449258\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.466016\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.529753\n",
      "\n",
      "Test set: Average loss: 0.5061, Accuracy: 8926/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        my_opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        my_opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    tst_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "#             print(y_batch_pred)\n",
    "#             print(y_batch)\n",
    "#             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            tst_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        tst_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(tst_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                     correct, \n",
    "                                                                                     len(tst_loader.dataset),\n",
    "                                                                                     accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65667e",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564e1bf",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.3em;line-height:1.5em'>Early Stopping은 다음과 같은 방식으로 진행됩니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>(1) 매 epoch마다 train을 진행합니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>(2) 한 epoch에 대해 train이 끝나면 validation set에서 현재까지 학습된 모델로 loss를 계산합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>val_loss = loss_func(y_val, y_val_est)</span>\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>(3) 현재의 validation loss가 이제까지의 validation loss의 최소값보다 연속으로 n번 크게 되면 학습을 멈춘다</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>val_loss > min_val_loss (n consecutive times) --> stop training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8f910",
   "metadata": {},
   "source": [
    "## 구현해봅시다.\n",
    "### 원래 validation set을 따로 만들어야 하지만, 여기선 편의상 test set을 validation set으로 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9f1d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**8\n",
    "trn_loader = DataLoader(trn_dset, batch_size = batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(tst_dset, batch_size = batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d759c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet().to(device)\n",
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-4) #L2 penalty에 들어가는 람다 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462d831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.499798\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.287806\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.240356\n",
      "val_loss(0.2937) < min_val_loss(inf)\n",
      ">> keep training, min_val_loss is replaced to 0.2937\n",
      "val set: Average loss: 0.2937, Accuracy: 9115/10000 (91%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.277243\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.162402\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.148435\n",
      "val_loss(0.2129) < min_val_loss(0.2937)\n",
      ">> keep training, min_val_loss is replaced to 0.2129\n",
      "val set: Average loss: 0.2129, Accuracy: 9362/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.162553\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.158954\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.193043\n",
      "val_loss(0.1761) < min_val_loss(0.2129)\n",
      ">> keep training, min_val_loss is replaced to 0.1761\n",
      "val set: Average loss: 0.1761, Accuracy: 9481/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.200137\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.157200\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.203530\n",
      "val_loss(0.1527) < min_val_loss(0.1761)\n",
      ">> keep training, min_val_loss is replaced to 0.1527\n",
      "val set: Average loss: 0.1527, Accuracy: 9538/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.074962\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.097990\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.212740\n",
      "val_loss(0.1369) < min_val_loss(0.1527)\n",
      ">> keep training, min_val_loss is replaced to 0.1369\n",
      "val set: Average loss: 0.1369, Accuracy: 9575/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.118355\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.146674\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.104442\n",
      "val_loss(0.1259) < min_val_loss(0.1369)\n",
      ">> keep training, min_val_loss is replaced to 0.1259\n",
      "val set: Average loss: 0.1259, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.102027\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.096251\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.060683\n",
      "val_loss(0.1159) < min_val_loss(0.1259)\n",
      ">> keep training, min_val_loss is replaced to 0.1159\n",
      "val set: Average loss: 0.1159, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.077138\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.064758\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.041251\n",
      "val_loss(0.1117) < min_val_loss(0.1159)\n",
      ">> keep training, min_val_loss is replaced to 0.1117\n",
      "val set: Average loss: 0.1117, Accuracy: 9657/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.125242\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.095996\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.097585\n",
      "val_loss(0.1032) < min_val_loss(0.1117)\n",
      ">> keep training, min_val_loss is replaced to 0.1032\n",
      "val set: Average loss: 0.1032, Accuracy: 9680/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.102473\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.081674\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.192568\n",
      "val_loss(0.0987) < min_val_loss(0.1032)\n",
      ">> keep training, min_val_loss is replaced to 0.0987\n",
      "val set: Average loss: 0.0987, Accuracy: 9692/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_val_loss = np.inf\n",
    "n_patience = 3\n",
    "n_violence = 0\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "        x_batch = x_batch.reshape(-1,784).to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        my_opt.zero_grad()\n",
    "        y_batch_prob = model(x_batch)\n",
    "        loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "        loss.backward()\n",
    "        my_opt.step()\n",
    "        if (batch_idx+1)%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(x_batch), \n",
    "                                                                           len(trn_loader.dataset),\n",
    "                                                                           100 * batch_idx / len(trn_loader),\n",
    "                                                                           loss.item()))\n",
    "    # 매 epoch이 끝날때 결과 찍기\n",
    "    print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                   len(trn_loader.dataset), \n",
    "                                                                   len(trn_loader.dataset),\n",
    "                                                                loss.item()))\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    y_real_list = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(val_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_batch_prob = model(x_batch)\n",
    "            y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "            val_loss += loss\n",
    "            \n",
    "            y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "            y_real_list.append(y_batch.detach().numpy())\n",
    "            \n",
    "        y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "        y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "        val_loss /= y_real.shape[0]\n",
    "        correct  = np.sum(y_real == y_pred)\n",
    "        accuracy = 100*correct / len(val_loader.dataset)\n",
    "        \n",
    "        \n",
    "    if val_loss < min_val_loss:\n",
    "        print(f'val_loss({val_loss:.4f}) < min_val_loss({min_val_loss:.4f})')\n",
    "        print(f'>> keep training, min_val_loss is replaced to {val_loss:.4f}')\n",
    "        min_val_loss = val_loss\n",
    "        n_violence = 0\n",
    "    else:\n",
    "        print(f'val_loss({val_loss:.4f}) >= min_val_loss({min_val_loss:.4f})')\n",
    "        print(f'>> n_violence is increased. n_violence={n_violence}')\n",
    "        n_violence+=1\n",
    "        \n",
    "    print('val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(val_loss, \n",
    "                                                                            correct, \n",
    "                                                                            len(val_loader.dataset),\n",
    "                                                                            accuracy))\n",
    "    \n",
    "    if n_violence >= n_patience:\n",
    "        print(f'>> n_violence={n_patience}. Stop training!\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322b765",
   "metadata": {},
   "source": [
    "# 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1d481",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.3em;line-height:1.5em'>1. Dropout의 비율을 변화시키면서, train_loss, test_loss, test_accuracy가 어떻게 바뀌는지 서술하세요.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94e900-9726-4bea-ab24-9c56c665ca42",
   "metadata": {},
   "source": [
    "#### MyNet에 drop_rate parameter추가\n",
    "- dropout비율을 0.25, 0.5, 0.75, 1.0으로 각각 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29e6efe-817c-4b9b-92de-736e252b9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**8\n",
    "trn_loader = DataLoader(trn_dset, batch_size = batch_size, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa77fa1-022d-4863-b642-f3c8e774915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,drop_rate):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        self.dropout = nn.Dropout(drop_rate) \n",
    "        self.apply(self._init_weights) \n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Linear): \n",
    "            nn.init.kaiming_normal_(submodule.weight) \n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) \n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # F.cross_entropy = F.log_softmax + F.nll_loss\n",
    "        # 뒤에서 cross_entropy를 사용하려면, 여기서 softmax 빼야됩니다.\n",
    "        result = F.log_softmax(x, dim=1) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edf764f5-1452-4cf5-9162-388a872b72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate_li = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "model_li = [MyNet(idx) for idx in drop_rate_li]\n",
    "my_opt_li = [optim.Adam(params = model.parameters(), lr = 2e-4) for model in model_li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3ba860c-23b4-4e10-b5b3-c14cc1b9f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.25, inplace=False) Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0002\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Dropout(p=0.5, inplace=False) Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0002\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Dropout(p=0.75, inplace=False) Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0002\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Dropout(p=1.0, inplace=False) Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0002\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for model, my_opt in zip(model_li, my_opt_li):\n",
    "    print(model.dropout, my_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ce9a912-84be-499c-bdaa-69f5f0d8e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Start model_Dropout(p=0.25, inplace=False)------------\n",
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.181400\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.142391\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.140403\n",
      "\n",
      "Test set: Average loss: 0.1070, Accuracy: 9684/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.172490\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.076932\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.121615\n",
      "\n",
      "Test set: Average loss: 0.1040, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.155239\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.088759\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.253389\n",
      "\n",
      "Test set: Average loss: 0.0999, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.120615\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.111701\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.089607\n",
      "\n",
      "Test set: Average loss: 0.0978, Accuracy: 9702/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.118176\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.092214\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.186248\n",
      "\n",
      "Test set: Average loss: 0.0934, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.107395\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.162069\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.175417\n",
      "\n",
      "Test set: Average loss: 0.0912, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.169814\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.119766\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.042150\n",
      "\n",
      "Test set: Average loss: 0.0894, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.112283\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.111166\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.069076\n",
      "\n",
      "Test set: Average loss: 0.0893, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.071736\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.078457\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.146466\n",
      "\n",
      "Test set: Average loss: 0.0878, Accuracy: 9743/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.110480\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.101366\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.156934\n",
      "\n",
      "Test set: Average loss: 0.0865, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "------------Start model_Dropout(p=0.5, inplace=False)------------\n",
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 0.359814\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 0.188469\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 0.460530\n",
      "\n",
      "Test set: Average loss: 0.1814, Accuracy: 9465/10000 (95%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.282503\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.396080\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 0.251585\n",
      "\n",
      "Test set: Average loss: 0.1763, Accuracy: 9470/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.280926\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.348531\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 0.223122\n",
      "\n",
      "Test set: Average loss: 0.1687, Accuracy: 9498/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.294881\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.306792\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 0.342388\n",
      "\n",
      "Test set: Average loss: 0.1659, Accuracy: 9512/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.242421\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.283686\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 0.183036\n",
      "\n",
      "Test set: Average loss: 0.1602, Accuracy: 9534/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.245202\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.355053\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 0.154503\n",
      "\n",
      "Test set: Average loss: 0.1550, Accuracy: 9552/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.323185\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.319668\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 0.355471\n",
      "\n",
      "Test set: Average loss: 0.1512, Accuracy: 9569/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.257062\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.426231\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 0.353694\n",
      "\n",
      "Test set: Average loss: 0.1493, Accuracy: 9574/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 0.293946\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.296125\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 0.444283\n",
      "\n",
      "Test set: Average loss: 0.1439, Accuracy: 9581/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.227147\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.248282\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.187778\n",
      "\n",
      "Test set: Average loss: 0.1435, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "------------Start model_Dropout(p=0.75, inplace=False)------------\n",
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 1.460937\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 1.183209\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 1.354302\n",
      "\n",
      "Test set: Average loss: 0.7464, Accuracy: 8787/10000 (88%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 1.391871\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 1.230563\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 1.470310\n",
      "\n",
      "Test set: Average loss: 0.6824, Accuracy: 8870/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 1.290876\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 1.251260\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 1.059666\n",
      "\n",
      "Test set: Average loss: 0.6549, Accuracy: 8882/10000 (89%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 1.147828\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 1.141800\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 1.313899\n",
      "\n",
      "Test set: Average loss: 0.6028, Accuracy: 8975/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 1.142760\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 1.222015\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 1.182359\n",
      "\n",
      "Test set: Average loss: 0.5836, Accuracy: 9008/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 1.197431\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 1.227883\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 1.270222\n",
      "\n",
      "Test set: Average loss: 0.5785, Accuracy: 9012/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 1.181649\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 1.098331\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 1.033792\n",
      "\n",
      "Test set: Average loss: 0.5330, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 1.200068\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 1.016638\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 1.106939\n",
      "\n",
      "Test set: Average loss: 0.5137, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 1.081406\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 1.010334\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 1.080918\n",
      "\n",
      "Test set: Average loss: 0.5060, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 1.094410\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 1.161702\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 0.862646\n",
      "\n",
      "Test set: Average loss: 0.4935, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "------------Start model_Dropout(p=1.0, inplace=False)------------\n",
      "Train Epoch: 0 [25344/60000 (42%)]\tLoss: 2.300168\n",
      "Train Epoch: 0 [50944/60000 (85%)]\tLoss: 2.303627\n",
      "Train Epoch: 0 [60000/60000 (100%)]\tLoss: 2.303593\n",
      "\n",
      "Test set: Average loss: 3.3502, Accuracy: 949/10000 (9%)\n",
      "\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 2.301807\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 2.299827\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 2.304588\n",
      "\n",
      "Test set: Average loss: 3.3526, Accuracy: 948/10000 (9%)\n",
      "\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 2.301929\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 2.300041\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLoss: 2.304534\n",
      "\n",
      "Test set: Average loss: 3.3544, Accuracy: 949/10000 (9%)\n",
      "\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 2.301779\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 2.299084\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLoss: 2.300776\n",
      "\n",
      "Test set: Average loss: 3.3559, Accuracy: 947/10000 (9%)\n",
      "\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 2.296644\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 2.297796\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLoss: 2.300580\n",
      "\n",
      "Test set: Average loss: 3.3571, Accuracy: 946/10000 (9%)\n",
      "\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 2.296755\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 2.300056\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLoss: 2.306099\n",
      "\n",
      "Test set: Average loss: 3.3581, Accuracy: 947/10000 (9%)\n",
      "\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 2.307071\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 2.302357\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLoss: 2.301659\n",
      "\n",
      "Test set: Average loss: 3.3588, Accuracy: 947/10000 (9%)\n",
      "\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 2.297502\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 2.303623\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLoss: 2.301158\n",
      "\n",
      "Test set: Average loss: 3.3595, Accuracy: 947/10000 (9%)\n",
      "\n",
      "Train Epoch: 8 [25344/60000 (42%)]\tLoss: 2.300379\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 2.302147\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLoss: 2.300425\n",
      "\n",
      "Test set: Average loss: 3.3596, Accuracy: 946/10000 (9%)\n",
      "\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 2.304324\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 2.302277\n",
      "Train Epoch: 9 [60000/60000 (100%)]\tLoss: 2.308113\n",
      "\n",
      "Test set: Average loss: 3.3600, Accuracy: 946/10000 (9%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_flag = 0\n",
    "\n",
    "n_epochs = 10\n",
    "for model, my_opt in zip(model_li, my_opt_li):\n",
    "    print(f'------------Start model_{model.dropout}------------')\n",
    "    model_flag += 1\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(trn_loader):\n",
    "            x_batch = x_batch.reshape(-1,784).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            my_opt.zero_grad()\n",
    "            y_batch_prob = model(x_batch)\n",
    "            loss = F.nll_loss(y_batch_prob, y_batch)\n",
    "            loss.backward()\n",
    "            my_opt.step()\n",
    "            if (batch_idx+1)%100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                               batch_idx * len(x_batch), \n",
    "                                                                               len(trn_loader.dataset),\n",
    "                                                                               100 * batch_idx / len(trn_loader),\n",
    "                                                                               loss.item()))\n",
    "        # 매 epoch이 끝날때 결과 찍기\n",
    "        print('Train Epoch: {} [{}/{} (100%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                       len(trn_loader.dataset), \n",
    "                                                                       len(trn_loader.dataset),\n",
    "                                                                    loss.item()))\n",
    "        model.eval()\n",
    "        y_pred_list = []\n",
    "        y_real_list = []\n",
    "        tst_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x_batch, y_batch) in enumerate(tst_loader):\n",
    "                x_batch = x_batch.reshape(-1,784).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_batch_prob = model(x_batch)\n",
    "                y_batch_pred = np.argmax(y_batch_prob, axis=1)\n",
    "    #             print(y_batch_pred)\n",
    "    #             print(y_batch)\n",
    "    #             y_batch_pred = y_batch_prob.argmax(dim=1, keepdim=True)\n",
    "                loss = F.nll_loss(y_batch_prob, y_batch, reduction='sum')\n",
    "                tst_loss += loss\n",
    "\n",
    "                y_pred_list.append(y_batch_pred.detach().numpy())\n",
    "                y_real_list.append(y_batch.detach().numpy())\n",
    "\n",
    "            y_real = np.concatenate([x for x in y_real_list], axis=0)\n",
    "            y_pred = np.concatenate([x for x in y_pred_list], axis=0)\n",
    "            tst_loss /= y_real.shape[0]\n",
    "            correct  = np.sum(y_real == y_pred)\n",
    "            accuracy = 100*correct / len(tst_loader.dataset)\n",
    "\n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(tst_loss, \n",
    "                                                                                         correct, \n",
    "                                                                                         len(tst_loader.dataset),\n",
    "                                                                                         accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb8f3e-291a-4b47-8711-5beb7c553fee",
   "metadata": {},
   "source": [
    "#### 결과\n",
    "- 본 모델은 inference결과가 충분히 좋은 모델이며 Dropout의 비율을 높일수록 규제를 높이는 것이기 때문에 train loss도 올라갈 뿐만 아니라 test accuracy가 내려간다.\n",
    "- 그러나 overfitting이 충분한 model을 많은 epoch를 가지고 학습한다면 dropout의 비율을 올리면 효과는 더욱 좋아질 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf09763-0229-4bc0-b748-5c4d7b65747b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2c202-711b-47a0-b425-852226ff6fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
